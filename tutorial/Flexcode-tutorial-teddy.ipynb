{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `flexcode` package can be installed directly by cloning the Github repository:\n",
    "\n",
    "<code style=\"background:black;color:white\">git clone https://github.com/tpospisi/FlexCode.git </code> <br>\n",
    "<code style=\"background:black;color:white\">python setup.py install </code> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import flexcode\n",
    "from cdetools.cde_loss import cde_loss\n",
    "from matplotlib import pyplot as plt\n",
    "from flexcode.regression_models import RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we run the `wget` module to fetch the data from the [COINtoolbox Github repository](https://github.com/COINtoolbox/photoz_catalogues/tree/master/Teddy). <br>\n",
    "You can download the Teddy A and B manually if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "data_dir = 'data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print('\"data\" subfolder created')\n",
    "    \n",
    "_ = wget.download('https://github.com/COINtoolbox/photoz_catalogues/raw/master/Teddy/teddy_A', \n",
    "              out='data/teddy_A.txt')\n",
    "\n",
    "_ = wget.download('https://github.com/COINtoolbox/photoz_catalogues/raw/master/Teddy/teddy_B', \n",
    "              out='data/teddy_B.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines assume you have the datasets `Teddy A` and `Teddy B` in a subfolder of your current directory. By default this subfolder is `data`, but it can be changed below. The following function extract the information from the .txt file and generates numpy array. <br> <br>\n",
    "\n",
    "You can find the `Teddy A` and `Teddy B` dataset in the [COINtoolbox Github repository](https://github.com/COINtoolbox/photoz_catalogues/tree/master/Teddy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_teddy_data(filename, train_data, directory='data/'):\n",
    "    \n",
    "    full_data = []\n",
    "    outfiles = ('teddy_x_train.npy', 'teddy_z_train.npy') if train_data else ('teddy_x_test.npy', 'teddy_z_test.npy')\n",
    "    with open(filename) as fp:\n",
    "        full_lines = fp.readlines()\n",
    "        for line in full_lines:\n",
    "            if '#' in line:\n",
    "                continue\n",
    "            full_data.append([float(el) for el in line.strip().split(' ') if el])\n",
    "        fp.close()\n",
    "    \n",
    "    # Saving the formatted Teddy data\n",
    "    np.save(arr=np.array(full_data)[:, 7:12], file=directory + outfiles[0])\n",
    "    np.save(arr=np.array(full_data)[:, 6], file=directory + outfiles[1])\n",
    "    print('Extraction and Saving Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_teddy_data(filename='data/teddy_A.txt', train_data=True, directory='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_teddy_data(filename='data/teddy_B.txt', train_data=False, directory='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Teddy Cosmology Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\texttt{TEDDY}$ data are comprised of 4 datasets, generated by subsampling from the [SDSS DR12](https://www.sdss.org/dr12/). <br>\n",
    "The 4 datasets are named respectively A, B, C and D.\n",
    "\n",
    "We use dataset A for training and B for testing.<br>\n",
    "Data in these two datasets share the same underlying distribution, so training-based algorithms do not need any further adjustments. <br>\n",
    "For more information, consult the [TEDDY Github Repo](https://github.com/COINtoolbox/photoz_catalogues/tree/master/Teddy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets have around 74,000 spectroscopic samples in it. <br>\n",
    "We downsample both training and testing, including only the first 2,000 and 500 galaxies respectively. <br>\n",
    "Here we also use a validation set of 500 galaxies, taken from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_limit_points = 2000\n",
    "valid_limit_points = 500\n",
    "test_limit_points = 500\n",
    "\n",
    "x_train = np.load(file='data/teddy_x_train.npy')[:train_limit_points, :]\n",
    "x_validation = np.load(file='data/teddy_x_train.npy')[train_limit_points:train_limit_points + valid_limit_points, :]\n",
    "x_test = np.load(file='data/teddy_x_test.npy')[:test_limit_points, :]\n",
    "\n",
    "z_train = np.load(file='data/teddy_y_train.npy')[:train_limit_points]\n",
    "z_validation = np.load(file='data/teddy_y_train.npy')[train_limit_points:train_limit_points + valid_limit_points]\n",
    "z_test = np.load(file='data/teddy_y_test.npy')[:test_limit_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Flexcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As any $\\texttt{scikit-learn}$ model, with the first call we initialize the model. <br>\n",
    "We need to specify the following:\n",
    "* regression model hyper-parameters; here we include those for Random Forest:\n",
    "    - the number of trees `n_estimators`, \n",
    "    - the maximum depth of a tree with `max_depth` and \n",
    "    - the splitting criterion `criterion`.\n",
    "* we also pass in the basis system and the maximum number of basis we want Flexcode to consider. Currently, Flexcode will automatically select the best number of basis according to the training data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "criterion = 'mse'\n",
    "max_depth = 5\n",
    "\n",
    "max_basis = 31\n",
    "basis_system = 'cosine'\n",
    "\n",
    "model = flexcode.FlexCodeModel(RandomForest, max_basis=max_basis, basis_system=basis_system,\n",
    "                             regression_params={'max_depth': max_depth, 'n_estimators': n_estimators, \n",
    "                                                'criterion': criterion})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the model by using the `train` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, z_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction, we only need to specify the number of points `n_grid` in the  CDE support, i.e. the grid over which we want the CDE to be predicted. <br>\n",
    "Flexcode creates a grid with that number of points between the minimum and maximum of the response in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 1000\n",
    "cde_test, z_grid = model.predict(x_test, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Flexcode CDE predictions, `cde_test` is a numpy array, but the actual full conditional density estimates is completely identified by the `n_basis` coefficients. In other words, one can achieve any resolution by just storing `n_basis` floats. <br>\n",
    "Densities are also normalized, i.e. they integrate to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import simps\n",
    "\n",
    "print(type(cde_test), cde_test.shape)\n",
    "\n",
    "den_integral = simps(cde_test[0, :], x=np.linspace(model.z_min[0], model.z_max[0], n_grid))\n",
    "print('Integral of the first density integrates to: %.2f' % den_integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predicted CDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the CDE loss function importing the function from the [`cdetools` package](https://github.com/tpospisi/cdetools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_loss_val, std_cde_loss = cde_loss(cde_test, z_grid, z_test)\n",
    "print('CDE Loss: %4.2f \\pm %.2f' % (cde_loss_val, std_cde_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here visualize the first 12 CDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "for jj, cde_predicted in enumerate(cde_test[:12,:]):\n",
    "    ax = fig.add_subplot(3, 4, jj + 1)\n",
    "    plt.plot(z_grid, cde_predicted, label=r'$\\hat{p}(z| x_{\\rm obs})$')\n",
    "    plt.axvline(z_test[jj], color='red', label=r'$z_{\\rm obs}$')\n",
    "    plt.xticks(size=16)\n",
    "    plt.yticks(size=16)\n",
    "    plt.xlabel(r'Redshift $z$', size=20)\n",
    "    plt.ylabel('CDE', size=20)\n",
    "    plt.legend(loc='upper right', prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Basis Bumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basys systems can create artificial \"bumps\" in the conditional distribution. <br>\n",
    "Flexcode allows to remove those bumps by selecting a threshold in density. The best threshold is chosen according to its CDE loss. <br>\n",
    "We can use the method `tune` and pass an array of thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_vec = [5e-2, 1e-1, 5e-1]\n",
    "model.tune(x_validation, z_validation, bump_threshold_grid=threshold_vec, n_grid=n_grid)\n",
    "print('Best Bump Removal Threshold: %s' % model.bump_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model saves internally the best bump thresholds, and so the next time the `predict` method is called the density will be adjusted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_test, z_grid = model.predict(x_test, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_loss_val, std_cde_loss = cde_loss(cde_test, z_grid, z_test)\n",
    "print('CDE Loss: %4.2f \\pm %.2f' % (cde_loss_val, std_cde_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 20))\n",
    "for jj, cde_predicted in enumerate(cde_test[:12,:]):\n",
    "    ax = fig.add_subplot(3, 4, jj + 1)\n",
    "    plt.plot(z_grid, cde_predicted, label=r'$\\hat{p}(z| x_{\\rm obs})$')\n",
    "    plt.axvline(z_test[jj], color='red', label=r'$z_{\\rm obs}$')\n",
    "    plt.xticks(size=16)\n",
    "    plt.yticks(size=16)\n",
    "    plt.xlabel(r'Redshift $z$', size=20)\n",
    "    plt.ylabel('CDE', size=20)\n",
    "    plt.legend(loc='upper right', prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Custom Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flexcode can be used with any scikit-learn-compatible regression model. <br>\n",
    "Here we show that the internal implementation of XGBoost (Gradient Boosted Trees Regression) and the XGBoost regressor from the `xgboost` package via Flexcode `CustomModel` yield identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from flexcode.regression_models import XGBoost, CustomModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flexcode XGBoost Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterize model\n",
    "model = flexcode.FlexCodeModel(XGBoost, max_basis=31, basis_system=\"cosine\",\n",
    "                             regression_params={'max_depth': 3, 'learning_rate': 0.5, 'objective': 'reg:linear'})\n",
    "\n",
    "# Fit and tune model\n",
    "model.fit(x_train, z_train)\n",
    "cdes_predict_xgb, z_grid = model.predict(x_test, n_grid=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flexcode Custom Model, using `XGBRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model_c = flexcode.FlexCodeModel(CustomModel, max_basis=31, basis_system=\"cosine\",\n",
    "                                 regression_params={'max_depth': 3, 'learning_rate': 0.5, 'objective': 'reg:linear'},\n",
    "                                 custom_model=XGBRegressor)\n",
    "\n",
    "# Fit and tune model\n",
    "model_c.fit(x_train, z_train)\n",
    "cdes_predict_custom, z_grid = model_c.predict(x_test, n_grid=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the largest discrepancy between the two sets of predicted CDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(cdes_predict_custom - cdes_predict_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDE Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the PIT and HPD values over our estimated CDEs. <br>\n",
    "Functions to calculate both values can be found in the [`cdetools` package](https://github.com/tpospisi/cdetools).\n",
    "<br>\n",
    "We suggest to clone the Github repository and install it.\n",
    "\n",
    "<code style=\"background:black;color:white\">git clone https://github.com/tpospisi/cdetools.git </code> <br>\n",
    "<code style=\"background:black;color:white\">cd cdetools/python/ </code> <br>\n",
    "<code style=\"background:black;color:white\">python setup.py install </code> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdetools.hpd_coverage import hpd_coverage\n",
    "from cdetools.cdf_coverage import cdf_coverage\n",
    "from cdetools.plot_utils import plot_with_uniform_band\n",
    "\n",
    "# Computing the values\n",
    "z_grid = np.linspace(z_train.min(), z_train.max(), n_grid)\n",
    "pit_values = cdf_coverage(cde_test, z_grid, z_test)\n",
    "hpd_values = hpd_coverage(cde_test, z_grid, z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the number of values per each bin in the histogram under uniformity assumptions. <br>\n",
    "We look at the 99% CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pit = plot_with_uniform_band(values=pit_values, ci_level=0.99, x_label='PIT Values', n_bins=30)\n",
    "fig_pit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_hpd = plot_with_uniform_band(values=hpd_values, ci_level=0.99, x_label='HPD Values', n_bins=30)   \n",
    "fig_hpd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
